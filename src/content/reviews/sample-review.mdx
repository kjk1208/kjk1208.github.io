---
title: "Attention is All You Need — 요약과 메모"
date: "2025-08-16"
category: "NLP"
tags: ["Transformer", "Attention"]
summary: "Transformer 논문의 핵심 수식과 구조 요약, 사소한 구현 팁 몇 가지"
---

다음은 **Scaled Dot-Product Attention**의 정의입니다.

$$
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

또한 멀티헤드 어텐션은 아래와 같습니다.

$$
\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(head_1, \dots, head_h) W^O
$$

<Mermaid code={`
graph TD
  A[Input] -->|Embed| B[Positional Encoding]
  B --> C[Multi-Head Attention]
  C --> D[Feed Forward]
  D --> E[Output]
`}/>

```python
# 작은 팁: PyTorch에서 마스크를 float('-inf')로 바꿔 softmax 이전에 더하면 편합니다.
attn = (Q @ K.transpose(-1, -2)) / math.sqrt(d_k)
attn.masked_fill_(mask == 0, float('-inf'))
attn = attn.softmax(dim=-1) @ V
```
